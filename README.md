# Texas Hospital Readmission Prediction

This project demonstrates an end-to-end data science workflow for predicting 30-day hospital readmissions. It begins with raw synthetic patient data in FHIR format, processes it through an ETL pipeline into a structured database, engineers features, performs exploratory data analysis, and concludes with predictive modeling.

The entire environment is containerized with Docker for full reproducibility.

## Project Workflow

The project is organized into a series of Jupyter notebooks, designed to be run in sequence:

1.  **FHIR ETL (`1 FHIR ETL.ipynb`):** A parallelized ETL pipeline that parses thousands of raw FHIR JSON bundles. It extracts relevant clinical and demographic data, enriches records with financial information, and loads the clean data into a **DuckDB** database.
2.  **Validation (`2 CSV ETL.ipynb`):** This notebook loads the "ground truth" CSV data (generated by Synthea) into the database to validate the FHIR ETL process by comparing row counts and schemas.
3.  **Feature Engineering (`3 SQL Feature Engineering.ipynb`):** Using SQL queries against the DuckDB database, this notebook creates the analytical dataset. It engineers the target variable (`readmitted_within_30_days`) and builds a rich feature set including patient demographics, admission details, and historical clinical data.
4.  **Exploratory Data Analysis (`4 EDA.ipynb`):** An analysis of the final feature-engineered dataset to understand distributions, identify correlations, and explore relationships between key features and the readmission target.
5.  **Modeling (`5 Modeling.ipynb`):** This notebook covers the machine learning workflow. It trains, evaluates, and compares **LightGBM** and **CatBoost** models. It also addresses class imbalance by tuning the decision threshold and applying class weights to improve recall, and validates the final model using 5-fold cross-validation.

-----

## Next Steps: Deployment Vision 

While the notebooks serve as the development and research environment, the ultimate goal is to create a deployable, end-to-end application. The plan involves the following steps:

1.  **Refactor Code:** The Python logic from the ETL and feature engineering notebooks will be refactored into modular scripts (`.py` files) for production use.
2.  **Prediction API:** A REST API will be built using **FastAPI**. This API will load the trained CatBoost model and provide a prediction endpoint. This endpoint will accept new patient data (e.g., an `encounter_id`) and return a real-time readmission risk score.
3.  **User Interface:** A simple front-end application will be developed to provide a user-friendly way to interact with the prediction API.
4.  **Containerization:** The final FastAPI application and its UI will be containerized using Docker for seamless deployment and scalability.

-----

## Part I: Data Generation

The raw data is generated using **Synthea™**, an open-source patient population simulator. To replicate the dataset, execute the following command from the root of the Synthea project directory. This will generate data for 100,000 patients in Texas and export it in both FHIR and CSV formats.

```bash
java -jar synthea-with-dependencies.jar Texas -p 100000 -s 42 --exporter.fhir.use_us_core_ig true --exporter.csv.export true --exporter.fhir.export true
```

Place the generated `fhir` and `csv` output folders into the `data/` directory of this project.

-----

## Part II: Environment Setup

The project environment is managed with Docker to ensure consistency and ease of use.

#### 1\. Build the Docker Image

This command builds the image using the provided Dockerfile, which includes all necessary dependencies like Python, DuckDB, and Jupyter Lab.

```bash
docker build -t readmissions-app .
```

#### 2\. Run the Jupyter Lab Container

This command starts the Jupyter Lab server. It maps port 8888 for browser access and mounts the current project directory into the container's `/app` folder, allowing you to edit files locally.

```bash
docker run --rm -p 8888:8888 -v .:/app readmissions-app jupyter lab --ip=0.0.0.0 --port=8888 --allow-root --no-browser
```

You can then access the Jupyter environment by navigating to `http://localhost:8888` in your browser and using the token provided in your terminal output.

-----

## Project Structure

```
ReadmissionRiskAPI/
├── data/                 # Directory for raw data
│   ├── fhir/             # Raw FHIR JSON bundles from Synthea
│   └── csv/              # Raw CSV files from Synthea (for validation)
├── models/               # Saved, trained model objects
├── notebooks/            # Jupyter notebooks for the project workflow
├── output/               # Output from ETL and feature engineering steps
├── .gitignore            # Git ignore file
├── Dockerfile            # Defines the Docker container environment
├── README.md             # This file
└── requirements.txt      # Python dependencies for the project
```